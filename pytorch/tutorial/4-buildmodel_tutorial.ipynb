{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n",
        "# https://pytorch.org/tutorials/beginner/colab\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "[Learn the Basics](intro.html) ||\n",
        "[Quickstart](quickstart_tutorial.html) ||\n",
        "[Tensors](tensorqs_tutorial.html) ||\n",
        "[Datasets & DataLoaders](data_tutorial.html) ||\n",
        "[Transforms](transforms_tutorial.html) ||\n",
        "**Build Model** ||\n",
        "[Autograd](autogradqs_tutorial.html) ||\n",
        "[Optimization](optimization_tutorial.html) ||\n",
        "[Save & Load Model](saveloadrun_tutorial.html)\n",
        "\n",
        "# Build the Neural Network\n",
        "\n",
        "Neural networks comprise of layers/modules that perform operations on data.\n",
        "The [torch.nn](https://pytorch.org/docs/stable/nn.html) namespace provides all the building blocks you need to\n",
        "build your own neural network. Every module in PyTorch subclasses the [nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html).\n",
        "A neural network is a module itself that consists of other modules (layers). This nested structure allows for\n",
        "building and managing complex architectures easily.\n",
        "\n",
        "In the following sections, we'll build a neural network to classify images in the FashionMNIST dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Get Device for Training\n",
        "We want to be able to train our model on a hardware accelerator like the GPU or MPS,\n",
        "if available. Let's check to see if [torch.cuda](https://pytorch.org/docs/stable/notes/cuda.html)\n",
        "or [torch.backends.mps](https://pytorch.org/docs/stable/notes/mps.html) are available, otherwise we use the CPU.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cpu device\n"
          ]
        }
      ],
      "source": [
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define the Class\n",
        "We define our neural network by subclassing ``nn.Module``, and\n",
        "initialize the neural network layers in ``__init__``. Every ``nn.Module`` subclass implements\n",
        "the operations on input data in the ``forward`` method.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We create an instance of ``NeuralNetwork``, and move it to the ``device``, and print\n",
        "its structure.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = NeuralNetwork().to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To use the model, we pass it the input data. This executes the model's ``forward``,\n",
        "along with some [background operations](https://github.com/pytorch/pytorch/blob/270111b7b611d174967ed204776985cefca9c144/torch/nn/modules/module.py#L866).\n",
        "Do not call ``model.forward()`` directly!\n",
        "\n",
        "Calling the model on the input returns a 2-dimensional tensor with dim=0 corresponding to each output of 10 raw predicted values for each class, and dim=1 corresponding to the individual values of each output.\n",
        "We get the prediction probabilities by passing it through an instance of the ``nn.Softmax`` module.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted class: tensor([9]) \n",
            " tensor([[[3.9802e-01, 3.5150e-01, 2.4817e-01, 7.5466e-01, 5.4210e-01,\n",
            "          7.9902e-01, 8.1513e-01, 9.9671e-01, 2.7973e-01, 8.7392e-01,\n",
            "          7.1121e-01, 8.4089e-01, 5.7831e-01, 5.7237e-02, 2.2994e-01,\n",
            "          4.7072e-01, 9.3501e-01, 2.6951e-01, 3.9196e-01, 3.3296e-01,\n",
            "          9.8649e-01, 3.1162e-02, 6.0784e-01, 8.5882e-01, 5.6583e-01,\n",
            "          8.2709e-01, 4.0122e-01, 9.7144e-01],\n",
            "         [6.9258e-01, 9.9394e-01, 5.2918e-01, 6.9696e-01, 3.4551e-01,\n",
            "          8.4556e-01, 6.2378e-01, 2.1127e-01, 5.6929e-01, 3.8001e-01,\n",
            "          9.2920e-01, 2.3528e-01, 9.1875e-01, 8.3831e-01, 5.6708e-01,\n",
            "          4.7983e-01, 4.3303e-01, 1.0929e-01, 7.9021e-01, 8.2096e-02,\n",
            "          9.9493e-01, 9.8227e-01, 3.3417e-01, 9.6130e-01, 8.5678e-01,\n",
            "          7.3834e-01, 5.8599e-01, 2.1394e-02],\n",
            "         [9.1809e-01, 2.4801e-01, 4.8416e-02, 1.3035e-01, 3.8421e-01,\n",
            "          2.6514e-01, 5.4354e-01, 6.1760e-01, 1.4484e-02, 9.3397e-02,\n",
            "          4.0467e-01, 2.5237e-01, 8.2304e-01, 7.8260e-01, 7.2441e-01,\n",
            "          2.4621e-01, 8.4098e-01, 1.9756e-01, 7.9577e-01, 9.1237e-01,\n",
            "          1.0649e-01, 1.4348e-01, 5.0068e-01, 7.3478e-01, 5.1808e-01,\n",
            "          6.8069e-01, 1.5038e-01, 5.8456e-01],\n",
            "         [6.7143e-01, 9.0470e-01, 4.9904e-01, 3.4927e-01, 5.7342e-01,\n",
            "          8.8205e-01, 7.2832e-01, 4.7006e-01, 2.0734e-01, 4.8270e-01,\n",
            "          3.2273e-01, 5.8526e-01, 4.0396e-01, 6.9415e-01, 9.7340e-01,\n",
            "          8.4273e-01, 4.4993e-01, 5.7146e-01, 4.3170e-01, 5.9099e-01,\n",
            "          6.9898e-01, 2.2440e-01, 6.1787e-01, 6.4646e-01, 8.5378e-01,\n",
            "          2.5414e-01, 7.6559e-01, 2.7886e-01],\n",
            "         [9.3959e-01, 1.2457e-01, 8.6924e-01, 4.4279e-01, 4.3390e-01,\n",
            "          3.0831e-01, 7.7641e-02, 2.3872e-01, 4.9815e-01, 3.6454e-01,\n",
            "          5.9684e-02, 7.5421e-01, 8.0565e-01, 6.1077e-01, 7.6869e-01,\n",
            "          2.6568e-01, 1.4622e-01, 4.0614e-01, 9.2837e-01, 9.2583e-01,\n",
            "          6.0679e-01, 1.4200e-01, 3.3324e-01, 2.5241e-01, 6.9024e-01,\n",
            "          3.9645e-01, 3.9172e-01, 2.3473e-01],\n",
            "         [2.3525e-01, 5.7173e-01, 1.3819e-02, 8.4059e-01, 8.5153e-01,\n",
            "          4.0019e-01, 7.7081e-01, 9.4246e-01, 6.8339e-01, 8.2670e-01,\n",
            "          2.5660e-01, 1.8921e-02, 6.2749e-01, 1.1343e-01, 8.5577e-01,\n",
            "          3.6464e-01, 3.3533e-01, 5.0326e-01, 1.6503e-01, 5.3366e-03,\n",
            "          5.1351e-01, 9.3249e-01, 6.2607e-01, 9.8440e-01, 2.4381e-01,\n",
            "          9.8761e-01, 2.7020e-01, 8.5190e-01],\n",
            "         [1.4454e-01, 2.6665e-01, 8.2405e-01, 2.1958e-01, 2.6906e-02,\n",
            "          2.0049e-01, 1.5209e-01, 5.6668e-01, 3.3476e-01, 8.5750e-02,\n",
            "          3.2670e-01, 6.0503e-01, 8.3389e-01, 6.3249e-01, 3.5694e-02,\n",
            "          6.4299e-01, 2.0296e-01, 9.6411e-01, 1.1653e-01, 6.9870e-01,\n",
            "          8.6621e-01, 2.9680e-01, 8.2204e-01, 4.1015e-02, 6.3649e-02,\n",
            "          5.1805e-01, 7.9354e-01, 7.1512e-01],\n",
            "         [5.7921e-01, 6.0539e-01, 3.5686e-01, 5.4136e-01, 7.7422e-01,\n",
            "          4.1966e-01, 2.1481e-01, 9.2634e-01, 5.1534e-02, 8.8052e-01,\n",
            "          5.5927e-04, 5.3147e-01, 1.8747e-01, 1.9657e-01, 4.9795e-01,\n",
            "          4.3542e-01, 2.4099e-02, 6.6025e-01, 6.8518e-01, 9.2994e-01,\n",
            "          4.1313e-02, 8.6885e-01, 3.8254e-01, 9.2700e-01, 9.4093e-01,\n",
            "          6.8661e-02, 1.5056e-02, 6.3944e-01],\n",
            "         [3.3066e-01, 2.3867e-01, 5.7918e-01, 4.2306e-01, 2.8599e-01,\n",
            "          4.5587e-01, 1.0523e-01, 6.3906e-01, 8.2742e-01, 1.2376e-01,\n",
            "          7.0432e-01, 2.7185e-01, 8.0181e-01, 5.8309e-01, 7.3452e-01,\n",
            "          8.7578e-02, 2.1237e-02, 4.4181e-01, 8.1603e-02, 5.7562e-01,\n",
            "          5.6287e-01, 8.1783e-01, 9.0176e-01, 6.1270e-01, 9.6378e-01,\n",
            "          8.0821e-01, 9.2143e-01, 7.0203e-01],\n",
            "         [7.3586e-01, 7.1197e-01, 9.2122e-01, 6.0927e-01, 7.7319e-01,\n",
            "          5.8553e-01, 2.7731e-01, 3.9155e-01, 4.1992e-01, 4.6218e-01,\n",
            "          4.5010e-01, 4.3804e-01, 7.7137e-01, 2.5647e-01, 6.4707e-01,\n",
            "          6.7253e-01, 5.3976e-01, 1.9583e-01, 3.0114e-01, 3.0349e-01,\n",
            "          5.1226e-01, 8.7245e-01, 3.5834e-01, 9.0617e-01, 5.2302e-02,\n",
            "          6.8933e-01, 5.7081e-01, 7.1705e-01],\n",
            "         [2.0361e-01, 6.2667e-01, 9.5517e-02, 9.8591e-01, 1.4364e-01,\n",
            "          9.4353e-01, 4.4976e-01, 5.5712e-02, 1.9579e-01, 4.0830e-02,\n",
            "          3.0997e-01, 9.2670e-01, 1.3392e-01, 7.3721e-01, 5.6603e-02,\n",
            "          3.3381e-01, 5.8154e-02, 3.4514e-01, 3.5892e-01, 8.2830e-01,\n",
            "          8.2659e-01, 8.3250e-01, 7.4832e-01, 5.9086e-02, 5.1453e-01,\n",
            "          2.4028e-01, 4.4537e-01, 8.1567e-01],\n",
            "         [3.2813e-01, 1.5694e-02, 6.8093e-01, 8.8483e-01, 8.5688e-01,\n",
            "          2.3306e-01, 8.0358e-01, 1.6055e-01, 9.4294e-02, 7.4592e-02,\n",
            "          9.0092e-03, 8.2316e-01, 1.4491e-01, 2.3883e-01, 9.7100e-01,\n",
            "          9.9066e-01, 2.7224e-01, 5.8554e-01, 4.7041e-01, 8.8111e-01,\n",
            "          1.2904e-02, 5.3050e-01, 6.6951e-01, 7.7771e-01, 7.4819e-01,\n",
            "          5.2717e-01, 9.1375e-01, 7.1888e-01],\n",
            "         [8.1185e-01, 1.3072e-01, 9.4759e-01, 7.8444e-01, 1.8226e-01,\n",
            "          2.2974e-01, 7.7324e-01, 3.3324e-01, 5.3286e-01, 8.1972e-03,\n",
            "          2.8771e-01, 9.0812e-01, 5.1802e-01, 5.8551e-01, 5.0061e-01,\n",
            "          6.4490e-01, 6.3619e-01, 6.7774e-01, 9.7144e-01, 1.1404e-01,\n",
            "          9.8444e-02, 9.1072e-01, 1.2794e-01, 1.6700e-01, 7.0423e-01,\n",
            "          6.6760e-01, 3.0616e-01, 7.4188e-01],\n",
            "         [7.8264e-01, 7.3793e-01, 4.2044e-01, 2.6154e-01, 8.1582e-01,\n",
            "          6.0820e-01, 5.8459e-01, 4.3353e-01, 6.8098e-01, 9.8812e-01,\n",
            "          2.8817e-01, 4.6245e-01, 3.4175e-01, 5.0297e-01, 8.2617e-01,\n",
            "          7.9270e-01, 4.3021e-01, 1.3751e-01, 4.5757e-01, 3.3562e-01,\n",
            "          7.5936e-02, 9.4941e-02, 6.3876e-02, 7.6786e-01, 2.3073e-01,\n",
            "          4.9805e-01, 1.4320e-01, 3.4014e-01],\n",
            "         [7.1722e-01, 2.9668e-01, 6.7288e-01, 9.3731e-01, 4.3066e-01,\n",
            "          1.6252e-01, 7.0140e-01, 2.1523e-01, 3.2862e-01, 6.5483e-01,\n",
            "          4.1891e-01, 5.6776e-01, 8.5152e-01, 2.3327e-01, 4.4260e-03,\n",
            "          2.5305e-01, 9.9060e-02, 1.8208e-01, 7.7252e-01, 7.1336e-01,\n",
            "          5.9511e-01, 3.0208e-01, 4.7320e-01, 9.9451e-01, 3.6411e-01,\n",
            "          5.9281e-01, 3.8778e-01, 1.8999e-01],\n",
            "         [8.3905e-01, 2.8654e-01, 1.1029e-01, 6.8167e-01, 6.1238e-01,\n",
            "          8.8710e-01, 7.6279e-01, 9.0869e-01, 6.2789e-01, 2.7794e-01,\n",
            "          2.3652e-01, 2.0218e-01, 8.0703e-01, 2.6157e-01, 3.9612e-02,\n",
            "          6.7831e-01, 4.9862e-01, 1.7909e-01, 4.1690e-01, 3.9953e-01,\n",
            "          4.7866e-01, 5.3528e-01, 2.1495e-01, 7.1340e-02, 5.1284e-01,\n",
            "          1.6140e-01, 2.5243e-01, 1.6537e-01],\n",
            "         [4.7543e-03, 9.1449e-01, 1.0982e-01, 3.0052e-01, 2.2198e-01,\n",
            "          3.9692e-01, 9.9761e-01, 7.6798e-01, 5.1037e-02, 6.4922e-01,\n",
            "          3.5075e-01, 4.6457e-01, 2.5352e-01, 9.3319e-01, 1.5263e-01,\n",
            "          4.6012e-01, 9.4623e-02, 3.7164e-01, 5.5765e-01, 6.3869e-01,\n",
            "          5.1743e-01, 1.6810e-01, 2.4166e-01, 9.5394e-01, 5.0707e-01,\n",
            "          9.2246e-01, 8.8756e-02, 7.8937e-01],\n",
            "         [6.7839e-01, 9.2741e-02, 3.2356e-01, 7.3621e-01, 8.0614e-01,\n",
            "          5.8164e-01, 2.3110e-01, 6.9322e-01, 8.9045e-01, 1.2433e-01,\n",
            "          1.0379e-02, 7.1340e-01, 5.4325e-01, 8.9872e-01, 8.8954e-01,\n",
            "          1.8757e-01, 6.7720e-02, 1.1334e-01, 1.5266e-02, 9.5867e-01,\n",
            "          5.5967e-01, 8.6037e-01, 5.1131e-01, 8.8446e-01, 9.1712e-02,\n",
            "          8.9926e-01, 4.3640e-01, 6.2760e-01],\n",
            "         [6.1406e-02, 6.9236e-02, 7.7506e-01, 8.0117e-02, 9.1436e-02,\n",
            "          8.4421e-01, 4.6018e-01, 7.3181e-01, 7.0634e-01, 5.5619e-01,\n",
            "          1.2237e-01, 1.8066e-01, 2.7376e-01, 3.4461e-01, 6.9827e-01,\n",
            "          8.0291e-01, 3.1642e-01, 4.8177e-01, 6.3687e-01, 2.2612e-01,\n",
            "          7.3532e-01, 8.1742e-01, 1.1100e-01, 5.0534e-01, 8.3428e-01,\n",
            "          5.5064e-01, 8.5950e-01, 3.8928e-01],\n",
            "         [8.1613e-01, 1.9850e-02, 4.4042e-01, 4.0060e-01, 8.4637e-01,\n",
            "          7.3467e-01, 1.7656e-01, 9.0099e-01, 3.1086e-01, 9.3334e-01,\n",
            "          2.9588e-02, 5.8298e-01, 4.6349e-01, 6.3701e-01, 1.2656e-01,\n",
            "          5.2552e-01, 4.8598e-01, 6.5543e-01, 5.0144e-01, 7.1139e-01,\n",
            "          5.6875e-01, 3.0350e-01, 7.6760e-02, 1.1435e-01, 8.2953e-01,\n",
            "          6.3908e-01, 7.2406e-01, 9.5309e-01],\n",
            "         [4.5712e-01, 6.7352e-01, 2.1705e-01, 1.9959e-01, 3.3470e-01,\n",
            "          7.7016e-01, 9.4500e-01, 9.0216e-01, 8.7126e-01, 3.1170e-01,\n",
            "          8.9287e-01, 1.0279e-01, 3.9303e-01, 3.4952e-01, 4.6828e-01,\n",
            "          1.0448e-01, 2.9932e-01, 2.5240e-01, 4.0750e-01, 7.2960e-01,\n",
            "          4.8075e-01, 3.9782e-01, 8.3043e-01, 1.0643e-01, 9.0015e-01,\n",
            "          3.4231e-01, 7.7552e-02, 2.4685e-01],\n",
            "         [1.3635e-01, 8.4164e-01, 5.6856e-01, 3.0109e-01, 9.4821e-01,\n",
            "          1.5387e-01, 6.7266e-01, 6.0842e-01, 7.4920e-01, 2.1633e-01,\n",
            "          4.7017e-01, 4.7714e-01, 5.2337e-01, 2.1985e-02, 7.5989e-01,\n",
            "          8.5129e-01, 1.6686e-01, 8.2442e-01, 8.8724e-01, 4.4489e-01,\n",
            "          3.1036e-01, 3.1899e-01, 5.1073e-01, 2.6597e-01, 2.3302e-01,\n",
            "          4.7986e-01, 3.0322e-01, 6.7628e-01],\n",
            "         [4.8054e-01, 4.6098e-01, 8.7049e-01, 4.9345e-02, 4.8551e-01,\n",
            "          9.2338e-01, 6.3212e-01, 4.4587e-01, 2.9543e-01, 3.9212e-01,\n",
            "          8.3596e-01, 1.6717e-01, 5.4759e-01, 9.9710e-01, 2.2088e-02,\n",
            "          8.0332e-01, 4.5058e-01, 8.6824e-01, 4.3300e-02, 7.1981e-02,\n",
            "          7.1344e-01, 6.1371e-01, 1.7753e-02, 6.1863e-01, 6.8927e-01,\n",
            "          4.5217e-01, 5.2042e-01, 6.8313e-01],\n",
            "         [2.7618e-01, 6.0184e-01, 4.5332e-01, 1.5569e-01, 5.8090e-02,\n",
            "          7.2814e-01, 5.8142e-01, 6.1359e-01, 7.3285e-01, 2.2526e-01,\n",
            "          4.2997e-01, 4.0351e-01, 9.8499e-01, 7.9712e-01, 8.0031e-01,\n",
            "          7.0334e-01, 7.5154e-01, 9.0762e-01, 2.4374e-02, 5.8145e-02,\n",
            "          9.0367e-01, 5.2487e-01, 9.0526e-01, 2.6942e-01, 9.4013e-01,\n",
            "          4.7571e-01, 9.0093e-01, 9.8749e-01],\n",
            "         [5.6912e-02, 2.0928e-01, 4.7098e-01, 1.7971e-01, 1.0175e-01,\n",
            "          8.5284e-01, 9.3625e-01, 5.5627e-02, 8.9415e-01, 5.4529e-01,\n",
            "          3.3406e-01, 4.9894e-01, 1.5798e-02, 8.2292e-02, 5.3299e-01,\n",
            "          3.9503e-01, 6.5317e-03, 1.1380e-01, 7.6462e-02, 4.9196e-01,\n",
            "          8.9528e-01, 2.0417e-01, 3.8741e-01, 8.5263e-01, 4.3237e-01,\n",
            "          2.6710e-01, 9.9814e-02, 1.9217e-01],\n",
            "         [7.2532e-01, 3.9312e-01, 3.7425e-01, 9.7523e-01, 1.1580e-01,\n",
            "          8.5104e-01, 9.2939e-01, 5.6613e-01, 9.9259e-01, 9.0291e-01,\n",
            "          8.9126e-01, 9.8488e-01, 9.1361e-01, 1.7589e-01, 5.1458e-01,\n",
            "          1.9030e-01, 7.9201e-01, 5.4314e-01, 8.2135e-01, 7.0080e-01,\n",
            "          6.2480e-01, 3.9773e-01, 5.3147e-01, 5.9727e-01, 8.9586e-01,\n",
            "          6.8446e-01, 5.6883e-01, 4.4344e-01],\n",
            "         [5.9045e-01, 9.3405e-01, 8.7853e-01, 5.9537e-01, 7.0915e-01,\n",
            "          4.7484e-01, 5.1998e-01, 7.0657e-02, 7.6641e-01, 7.7979e-01,\n",
            "          7.3209e-01, 9.6664e-01, 1.9467e-01, 4.3173e-01, 6.9423e-01,\n",
            "          7.8506e-01, 7.7058e-02, 5.4823e-01, 2.2984e-01, 7.4472e-01,\n",
            "          2.5057e-01, 5.1021e-02, 4.7645e-01, 2.0171e-01, 8.6789e-01,\n",
            "          5.2338e-01, 8.4718e-01, 2.4227e-01],\n",
            "         [8.4037e-01, 6.3418e-01, 9.7014e-01, 9.5470e-01, 8.8306e-01,\n",
            "          5.1059e-01, 8.4035e-01, 6.5137e-01, 2.8615e-01, 8.1810e-01,\n",
            "          1.4324e-01, 2.7324e-01, 2.0071e-01, 8.3034e-01, 3.8781e-02,\n",
            "          9.5097e-01, 6.7897e-01, 4.5051e-01, 2.7694e-01, 1.6686e-01,\n",
            "          7.4709e-01, 3.4586e-01, 1.4610e-01, 2.7587e-01, 2.9528e-01,\n",
            "          5.4767e-01, 1.2740e-01, 8.0337e-01]]])\n"
          ]
        }
      ],
      "source": [
        "X = torch.rand(1, 28, 28, device=device)\n",
        "logits = model(X)\n",
        "pred_probab = nn.Softmax(dim=1)(logits)\n",
        "y_pred = pred_probab.argmax(1)\n",
        "print(f\"Predicted class: {y_pred} \\n {X}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "--------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Layers\n",
        "\n",
        "Let's break down the layers in the FashionMNIST model. To illustrate it, we\n",
        "will take a sample minibatch of 3 images of size 28x28 and see what happens to it as\n",
        "we pass it through the network.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([3, 28, 28])\n"
          ]
        }
      ],
      "source": [
        "input_image = torch.rand(3,28,28)\n",
        "print(input_image.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### nn.Flatten\n",
        "We initialize the [nn.Flatten](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html)\n",
        "layer to convert each 2D 28x28 image into a contiguous array of 784 pixel values (\n",
        "the minibatch dimension (at dim=0) is maintained).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([3, 784])\n"
          ]
        }
      ],
      "source": [
        "flatten = nn.Flatten()\n",
        "flat_image = flatten(input_image)\n",
        "\n",
        "print(flat_image.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### nn.Linear\n",
        "The [linear layer](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)\n",
        "is a module that applies a linear transformation on the input using its stored weights and biases.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([3, 20])\n"
          ]
        }
      ],
      "source": [
        "layer1 = nn.Linear(in_features=28*28, out_features=20)\n",
        "hidden1 = layer1(flat_image)\n",
        "print(hidden1.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### nn.ReLU\n",
        "Non-linear activations are what create the complex mappings between the model's inputs and outputs.\n",
        "They are applied after linear transformations to introduce *nonlinearity*, helping neural networks\n",
        "learn a wide variety of phenomena.\n",
        "\n",
        "In this model, we use [nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html) between our\n",
        "linear layers, but there's other activations to introduce non-linearity in your model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before ReLU: tensor([[ 1.6576e-01,  8.0559e-02, -1.0609e-01, -2.8955e-01,  2.3406e-01,\n",
            "          9.0722e-02, -5.4006e-01,  1.3332e-01, -6.8582e-04, -7.1086e-01,\n",
            "          2.2670e-01,  3.4781e-01,  3.6784e-01,  9.4104e-02, -4.4424e-03,\n",
            "         -5.3927e-01, -1.8189e-01,  3.1705e-01,  2.2195e-01, -5.5640e-01],\n",
            "        [ 7.2669e-01,  7.0936e-02, -2.9644e-01, -3.6701e-01,  3.5238e-01,\n",
            "         -9.3453e-02, -8.8480e-01, -1.4652e-01,  1.4381e-01, -6.2109e-01,\n",
            "          3.7545e-01, -7.3073e-03,  3.7699e-02,  3.4682e-01, -1.2071e-02,\n",
            "         -8.5902e-01,  5.7812e-02,  4.3451e-01,  6.6886e-01, -7.9684e-01],\n",
            "        [ 4.5013e-01, -9.1437e-02, -1.0453e-01, -1.6306e-01,  1.1860e-01,\n",
            "         -2.0632e-01, -5.3787e-01, -3.3522e-02, -1.0103e-01, -6.7470e-01,\n",
            "          1.0775e-01,  3.3737e-01,  2.8508e-01,  2.5667e-01,  3.0749e-02,\n",
            "         -7.9390e-01, -6.2882e-02,  3.0614e-01, -1.2466e-01, -6.4856e-01]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "\n",
            "\n",
            "After ReLU: tensor([[0.1658, 0.0806, 0.0000, 0.0000, 0.2341, 0.0907, 0.0000, 0.1333, 0.0000,\n",
            "         0.0000, 0.2267, 0.3478, 0.3678, 0.0941, 0.0000, 0.0000, 0.0000, 0.3171,\n",
            "         0.2219, 0.0000],\n",
            "        [0.7267, 0.0709, 0.0000, 0.0000, 0.3524, 0.0000, 0.0000, 0.0000, 0.1438,\n",
            "         0.0000, 0.3755, 0.0000, 0.0377, 0.3468, 0.0000, 0.0000, 0.0578, 0.4345,\n",
            "         0.6689, 0.0000],\n",
            "        [0.4501, 0.0000, 0.0000, 0.0000, 0.1186, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.1077, 0.3374, 0.2851, 0.2567, 0.0307, 0.0000, 0.0000, 0.3061,\n",
            "         0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n"
          ]
        }
      ],
      "source": [
        "print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
        "hidden1 = nn.ReLU()(hidden1)\n",
        "print(f\"After ReLU: {hidden1}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### nn.Sequential\n",
        "[nn.Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) is an ordered\n",
        "container of modules. The data is passed through all the modules in the same order as defined. You can use\n",
        "sequential containers to put together a quick network like ``seq_modules``.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[-0.1008, -0.2949, -0.2448, -0.0973,  0.0016, -0.2144, -0.0620,  0.0790,\n",
            "         -0.2253, -0.0201],\n",
            "        [-0.1245, -0.3494, -0.1992,  0.0033,  0.1088, -0.2210, -0.1391,  0.0316,\n",
            "         -0.2295,  0.0586],\n",
            "        [-0.1383, -0.3502, -0.2712, -0.0388,  0.1459, -0.3403, -0.1358,  0.0519,\n",
            "         -0.2875, -0.0030]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "seq_modules = nn.Sequential(\n",
        "    flatten,\n",
        "    layer1,\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(20, 10)\n",
        ")\n",
        "input_image = torch.rand(3,28,28)\n",
        "logits = seq_modules(input_image)\n",
        "print(logits)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### nn.Softmax\n",
        "The last linear layer of the neural network returns `logits` - raw values in [-\\infty, \\infty] - which are passed to the\n",
        "[nn.Softmax](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html) module. The logits are scaled to values\n",
        "[0, 1] representing the model's predicted probabilities for each class. ``dim`` parameter indicates the dimension along\n",
        "which the values must sum to 1.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.1010, 0.0832, 0.0875, 0.1014, 0.1119, 0.0902, 0.1050, 0.1209, 0.0892,\n",
            "         0.1095],\n",
            "        [0.0972, 0.0776, 0.0902, 0.1104, 0.1227, 0.0882, 0.0958, 0.1136, 0.0875,\n",
            "         0.1167],\n",
            "        [0.0985, 0.0797, 0.0862, 0.1088, 0.1309, 0.0805, 0.0987, 0.1191, 0.0848,\n",
            "         0.1128]], grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ],
      "source": [
        "softmax = nn.Softmax(dim=1)\n",
        "pred_probab = softmax(logits)\n",
        "\n",
        "print(pred_probab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Parameters\n",
        "Many layers inside a neural network are *parameterized*, i.e. have associated weights\n",
        "and biases that are optimized during training. Subclassing ``nn.Module`` automatically\n",
        "tracks all fields defined inside your model object, and makes all parameters\n",
        "accessible using your model's ``parameters()`` or ``named_parameters()`` methods.\n",
        "\n",
        "In this example, we iterate over each parameter, and print its size and a preview of its values.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model structure: NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n",
            "\n",
            "\n",
            "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[ 0.0249,  0.0056,  0.0351,  ...,  0.0321, -0.0303, -0.0231],\n",
            "        [ 0.0076, -0.0357,  0.0206,  ...,  0.0250, -0.0021,  0.0319]],\n",
            "       grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([-0.0077, -0.0321], grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[-0.0241,  0.0071, -0.0014,  ...,  0.0020, -0.0113,  0.0322],\n",
            "        [-0.0359, -0.0360, -0.0011,  ...,  0.0024,  0.0175, -0.0115]],\n",
            "       grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([-0.0038, -0.0434], grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[-0.0328, -0.0165,  0.0231,  ..., -0.0055,  0.0333, -0.0106],\n",
            "        [-0.0201, -0.0180,  0.0332,  ..., -0.0176, -0.0427,  0.0122]],\n",
            "       grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([0.0219, 0.0023], grad_fn=<SliceBackward0>) \n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(f\"Model structure: {model}\\n\\n\")\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "--------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Further Reading\n",
        "- [torch.nn API](https://pytorch.org/docs/stable/nn.html)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
