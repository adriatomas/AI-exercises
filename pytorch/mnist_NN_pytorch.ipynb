{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_neural_network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(My_neural_network, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 1000),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1000, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.FashionMNIST(\n",
    "  root=\"./tutorial/data\",\n",
    "  train=True,\n",
    "  download=False,\n",
    "  transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "  root=\"./tutorial/data\",\n",
    "  train=False,\n",
    "  download=False,\n",
    "  transform=ToTensor()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer, batch_size):\n",
    "  size = len(dataloader.dataset)\n",
    "\n",
    "  model.train()\n",
    "\n",
    "  for batch, (X, y) in enumerate(dataloader):\n",
    "    pred = model(X) #forward\n",
    "    loss = loss_fn(pred, y)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    optimizer.zero_grad() #reset all gradients\n",
    "\n",
    "    if batch % 20 == 0:\n",
    "      loss, current = loss.item(), batch * batch_size + len(X)\n",
    "      print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "  model.eval() # unnecessary in this situation but is a good practice\n",
    "  #eval() === Test()\n",
    "  size = len(dataloader.dataset)\n",
    "  num_batches = len(dataloader)\n",
    "  test_loss, correct = 0, 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for X, y in dataloader:\n",
    "      pred = model(X)\n",
    "      test_loss += loss_fn(pred, y).item()\n",
    "      correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "  test_loss /= num_batches\n",
    "  correct /= size\n",
    "  print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      " ---------------------------\n",
      "loss: 2.304055  [  500/60000]\n",
      "loss: 0.814735  [10500/60000]\n",
      "loss: 0.678321  [20500/60000]\n",
      "loss: 0.460713  [30500/60000]\n",
      "loss: 0.496757  [40500/60000]\n",
      "loss: 0.549582  [50500/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.5%, Avg loss: 0.490101 \n",
      "\n",
      "Epoch 2\n",
      " ---------------------------\n",
      "loss: 0.390742  [  500/60000]\n",
      "loss: 0.391729  [10500/60000]\n",
      "loss: 0.439607  [20500/60000]\n",
      "loss: 0.366113  [30500/60000]\n",
      "loss: 0.430365  [40500/60000]\n",
      "loss: 0.443983  [50500/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.462171 \n",
      "\n",
      "Epoch 3\n",
      " ---------------------------\n",
      "loss: 0.356696  [  500/60000]\n",
      "loss: 0.364639  [10500/60000]\n",
      "loss: 0.384052  [20500/60000]\n",
      "loss: 0.326943  [30500/60000]\n",
      "loss: 0.374211  [40500/60000]\n",
      "loss: 0.401074  [50500/60000]\n",
      "Test Error: \n",
      " Accuracy: 85.1%, Avg loss: 0.418693 \n",
      "\n",
      "Epoch 4\n",
      " ---------------------------\n",
      "loss: 0.338031  [  500/60000]\n",
      "loss: 0.350865  [10500/60000]\n",
      "loss: 0.383816  [20500/60000]\n",
      "loss: 0.298668  [30500/60000]\n",
      "loss: 0.387051  [40500/60000]\n",
      "loss: 0.398954  [50500/60000]\n",
      "Test Error: \n",
      " Accuracy: 85.1%, Avg loss: 0.410035 \n",
      "\n",
      "Epoch 5\n",
      " ---------------------------\n",
      "loss: 0.315671  [  500/60000]\n",
      "loss: 0.339427  [10500/60000]\n",
      "loss: 0.374468  [20500/60000]\n",
      "loss: 0.321887  [30500/60000]\n",
      "loss: 0.378192  [40500/60000]\n",
      "loss: 0.427934  [50500/60000]\n",
      "Test Error: \n",
      " Accuracy: 85.0%, Avg loss: 0.418572 \n",
      "\n",
      "Epoch 6\n",
      " ---------------------------\n",
      "loss: 0.332349  [  500/60000]\n",
      "loss: 0.330729  [10500/60000]\n",
      "loss: 0.337734  [20500/60000]\n",
      "loss: 0.293541  [30500/60000]\n",
      "loss: 0.352300  [40500/60000]\n",
      "loss: 0.409629  [50500/60000]\n",
      "Test Error: \n",
      " Accuracy: 85.3%, Avg loss: 0.428124 \n",
      "\n",
      "Epoch 7\n",
      " ---------------------------\n",
      "loss: 0.325115  [  500/60000]\n",
      "loss: 0.330915  [10500/60000]\n",
      "loss: 0.369639  [20500/60000]\n",
      "loss: 0.301031  [30500/60000]\n",
      "loss: 0.319679  [40500/60000]\n",
      "loss: 0.386976  [50500/60000]\n",
      "Test Error: \n",
      " Accuracy: 85.6%, Avg loss: 0.420165 \n",
      "\n",
      "Epoch 8\n",
      " ---------------------------\n",
      "loss: 0.302523  [  500/60000]\n",
      "loss: 0.301752  [10500/60000]\n",
      "loss: 0.346055  [20500/60000]\n",
      "loss: 0.285689  [30500/60000]\n",
      "loss: 0.361715  [40500/60000]\n",
      "loss: 0.373054  [50500/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.1%, Avg loss: 0.397530 \n",
      "\n",
      "Epoch 9\n",
      " ---------------------------\n",
      "loss: 0.291183  [  500/60000]\n",
      "loss: 0.337554  [10500/60000]\n",
      "loss: 0.375425  [20500/60000]\n",
      "loss: 0.300818  [30500/60000]\n",
      "loss: 0.329656  [40500/60000]\n",
      "loss: 0.378691  [50500/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.0%, Avg loss: 0.404215 \n",
      "\n",
      "Epoch 10\n",
      " ---------------------------\n",
      "loss: 0.295594  [  500/60000]\n",
      "loss: 0.339176  [10500/60000]\n",
      "loss: 0.358768  [20500/60000]\n",
      "loss: 0.293594  [30500/60000]\n",
      "loss: 0.354715  [40500/60000]\n",
      "loss: 0.400695  [50500/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.2%, Avg loss: 0.415061 \n",
      "\n",
      "Epoch 11\n",
      " ---------------------------\n",
      "loss: 0.308190  [  500/60000]\n",
      "loss: 0.301365  [10500/60000]\n",
      "loss: 0.336433  [20500/60000]\n",
      "loss: 0.301448  [30500/60000]\n",
      "loss: 0.333637  [40500/60000]\n",
      "loss: 0.412798  [50500/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.2%, Avg loss: 0.421608 \n",
      "\n",
      "Epoch 12\n",
      " ---------------------------\n",
      "loss: 0.306308  [  500/60000]\n",
      "loss: 0.298845  [10500/60000]\n",
      "loss: 0.346925  [20500/60000]\n",
      "loss: 0.291674  [30500/60000]\n",
      "loss: 0.335544  [40500/60000]\n",
      "loss: 0.367389  [50500/60000]\n",
      "Test Error: \n",
      " Accuracy: 85.7%, Avg loss: 0.425494 \n",
      "\n",
      "Epoch 13\n",
      " ---------------------------\n",
      "loss: 0.292837  [  500/60000]\n",
      "loss: 0.320351  [10500/60000]\n",
      "loss: 0.358234  [20500/60000]\n",
      "loss: 0.278113  [30500/60000]\n",
      "loss: 0.352620  [40500/60000]\n",
      "loss: 0.393592  [50500/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.0%, Avg loss: 0.425368 \n",
      "\n",
      "Epoch 14\n",
      " ---------------------------\n",
      "loss: 0.301618  [  500/60000]\n",
      "loss: 0.329160  [10500/60000]\n",
      "loss: 0.335405  [20500/60000]\n",
      "loss: 0.288392  [30500/60000]\n",
      "loss: 0.333067  [40500/60000]\n",
      "loss: 0.377363  [50500/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.4%, Avg loss: 0.408899 \n",
      "\n",
      "Epoch 15\n",
      " ---------------------------\n",
      "loss: 0.276108  [  500/60000]\n",
      "loss: 0.298005  [10500/60000]\n",
      "loss: 0.355593  [20500/60000]\n",
      "loss: 0.280037  [30500/60000]\n",
      "loss: 0.321191  [40500/60000]\n",
      "loss: 0.356989  [50500/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.3%, Avg loss: 0.418047 \n",
      "\n",
      "Epoch 16\n",
      " ---------------------------\n",
      "loss: 0.277340  [  500/60000]\n",
      "loss: 0.292070  [10500/60000]\n",
      "loss: 0.336953  [20500/60000]\n",
      "loss: 0.276103  [30500/60000]\n",
      "loss: 0.325995  [40500/60000]\n",
      "loss: 0.353766  [50500/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.4%, Avg loss: 0.413597 \n",
      "\n",
      "Epoch 17\n",
      " ---------------------------\n",
      "loss: 0.271196  [  500/60000]\n",
      "loss: 0.302679  [10500/60000]\n",
      "loss: 0.311839  [20500/60000]\n",
      "loss: 0.277429  [30500/60000]\n",
      "loss: 0.333320  [40500/60000]\n",
      "loss: 0.385490  [50500/60000]\n",
      "Test Error: \n",
      " Accuracy: 85.9%, Avg loss: 0.407967 \n",
      "\n",
      "Epoch 18\n",
      " ---------------------------\n",
      "loss: 0.302093  [  500/60000]\n",
      "loss: 0.319265  [10500/60000]\n",
      "loss: 0.322889  [20500/60000]\n",
      "loss: 0.260104  [30500/60000]\n",
      "loss: 0.303069  [40500/60000]\n",
      "loss: 0.341941  [50500/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.3%, Avg loss: 0.409316 \n",
      "\n",
      "Epoch 19\n",
      " ---------------------------\n",
      "loss: 0.285338  [  500/60000]\n",
      "loss: 0.310561  [10500/60000]\n",
      "loss: 0.333357  [20500/60000]\n",
      "loss: 0.268925  [30500/60000]\n",
      "loss: 0.329939  [40500/60000]\n",
      "loss: 0.364307  [50500/60000]\n",
      "Test Error: \n",
      " Accuracy: 85.9%, Avg loss: 0.422655 \n",
      "\n",
      "Epoch 20\n",
      " ---------------------------\n",
      "loss: 0.280534  [  500/60000]\n",
      "loss: 0.326846  [10500/60000]\n",
      "loss: 0.344057  [20500/60000]\n",
      "loss: 0.292188  [30500/60000]\n",
      "loss: 0.298192  [40500/60000]\n",
      "loss: 0.348220  [50500/60000]\n",
      "Test Error: \n",
      " Accuracy: 85.5%, Avg loss: 0.415508 \n",
      "\n",
      "done?\n"
     ]
    }
   ],
   "source": [
    "def main(): \n",
    "  batch_size = 500\n",
    "  learning_rate = 0.03\n",
    "  epochs = 20\n",
    "\n",
    "  train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "  test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "  model = My_neural_network()\n",
    "\n",
    "  loss_fn = nn.CrossEntropyLoss()\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}\\n ---------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer, batch_size)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "  \n",
    "\n",
    "main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
