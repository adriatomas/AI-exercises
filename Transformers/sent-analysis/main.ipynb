{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and setting CONSTANTS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6214/3255591236.py:3: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "NUM_TOKENS = 50257\n",
    "EMBEDDING_SIZE = 512\n",
    "HIDDEN_SIZE = 512\n",
    "TARGET_SIZE = 2\n",
    "BATCH_SIZE = 64\n",
    "MODEL_ID = \"projecte-aina/FLOR-6.3B\"\n",
    "NUM_LAYERS = 512\n",
    "MAX_LENGTH = 512\n",
    "# NUM_TOKENS = 10000\n",
    "# EMBEDDING_SIZE = 300\n",
    "# HIDDEN_SIZE = 300\n",
    "# TARGET_SIZE = 2\n",
    "# BATCH_SIZE = 20\n",
    "# MODEL_ID = \"projecte-aina/FLOR-6.3B\"\n",
    "# NUM_LAYERS = 107\n",
    "# MAX_LENGTH = 107\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom LSTM Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEmbedding(nn.Module):\n",
    "  def __init__(self, encoder, input_size, embedding_size, hidden_size, target_size):\n",
    "        \n",
    "        super(CustomEmbedding, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_embeddings=input_size, embedding_dim=embedding_size)   \n",
    "        self.encoder = encoder\n",
    "        self.lastlayer = nn.Linear(hidden_size, target_size)\n",
    "      \n",
    "       \n",
    "\n",
    "\n",
    "  def forward(self, sentence):\n",
    "      embeds = self.embedding(sentence)\n",
    "      trans_logit = self.encoder(embeds)\n",
    "      \n",
    "      return self.lastlayer(trans_logit)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoviesDataset(Dataset):\n",
    "    def __init__(self, x, y, model_max_length):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.model_max_length = model_max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.x[idx]\n",
    "        text_tokenized = tokenizer.encode(text, padding='max_length', max_length=self.model_max_length , truncation=True, return_tensors='pt', )\n",
    "        y_hot = nn.functional.one_hot(torch.tensor(self.y[idx]), num_classes=2)\n",
    "        \n",
    "        return text_tokenized[0], y_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "\n",
    "\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 10 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            # X, y = X.to(device), y.to(device)\n",
    "            \n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main method starting point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adria/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 4.662898  [    0/40000]\n",
      "loss: 1.115108  [  640/40000]\n",
      "loss: 1.657282  [ 1280/40000]\n",
      "loss: 5.726794  [ 1920/40000]\n",
      "loss: 0.938670  [ 2560/40000]\n",
      "loss: 1.151015  [ 3200/40000]\n",
      "loss: 0.789729  [ 3840/40000]\n",
      "loss: 0.790114  [ 4480/40000]\n",
      "loss: 0.950367  [ 5120/40000]\n",
      "loss: 0.864568  [ 5760/40000]\n",
      "loss: 0.750104  [ 6400/40000]\n",
      "loss: 0.960788  [ 7040/40000]\n",
      "loss: 0.994710  [ 7680/40000]\n",
      "loss: 0.792030  [ 8320/40000]\n",
      "loss: 0.750696  [ 8960/40000]\n",
      "loss: 0.722606  [ 9600/40000]\n",
      "loss: 0.766440  [10240/40000]\n",
      "loss: 0.838112  [10880/40000]\n",
      "loss: 0.969817  [11520/40000]\n",
      "loss: 0.895237  [12160/40000]\n",
      "loss: 0.733892  [12800/40000]\n",
      "loss: 1.065675  [13440/40000]\n",
      "loss: 0.756095  [14080/40000]\n",
      "loss: 1.049976  [14720/40000]\n",
      "loss: 0.860166  [15360/40000]\n",
      "loss: 1.136674  [16000/40000]\n",
      "loss: 1.069410  [16640/40000]\n",
      "loss: 0.802097  [17280/40000]\n",
      "loss: 0.768728  [17920/40000]\n",
      "loss: 0.732851  [18560/40000]\n",
      "loss: 0.927380  [19200/40000]\n",
      "loss: 0.726935  [19840/40000]\n",
      "loss: 0.726995  [20480/40000]\n",
      "loss: 0.732591  [21120/40000]\n",
      "loss: 0.790723  [21760/40000]\n",
      "loss: 0.903594  [22400/40000]\n",
      "loss: 0.858388  [23040/40000]\n",
      "loss: 0.782614  [23680/40000]\n",
      "loss: 1.001100  [24320/40000]\n",
      "loss: 1.229490  [24960/40000]\n",
      "loss: 0.720586  [25600/40000]\n",
      "loss: 0.801830  [26240/40000]\n",
      "loss: 0.862268  [26880/40000]\n",
      "loss: 1.056868  [27520/40000]\n",
      "loss: 0.826711  [28160/40000]\n",
      "loss: 0.792211  [28800/40000]\n",
      "loss: 0.859496  [29440/40000]\n",
      "loss: 0.841322  [30080/40000]\n",
      "loss: 0.722271  [30720/40000]\n",
      "loss: 0.756076  [31360/40000]\n",
      "loss: 0.760724  [32000/40000]\n",
      "loss: 0.797822  [32640/40000]\n",
      "loss: 0.743132  [33280/40000]\n",
      "loss: 0.731093  [33920/40000]\n",
      "loss: 0.747212  [34560/40000]\n",
      "loss: 0.777449  [35200/40000]\n",
      "loss: 0.879382  [35840/40000]\n",
      "loss: 0.888901  [36480/40000]\n",
      "loss: 0.728533  [37120/40000]\n",
      "loss: 1.256348  [37760/40000]\n",
      "loss: 0.865475  [38400/40000]\n",
      "loss: 0.756753  [39040/40000]\n",
      "loss: 0.766612  [39680/40000]\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.901325 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.709623  [    0/40000]\n",
      "loss: 0.733955  [  640/40000]\n",
      "loss: 0.827197  [ 1280/40000]\n",
      "loss: 0.815815  [ 1920/40000]\n",
      "loss: 0.813625  [ 2560/40000]\n",
      "loss: 0.780637  [ 3200/40000]\n",
      "loss: 0.787688  [ 3840/40000]\n",
      "loss: 0.891567  [ 4480/40000]\n",
      "loss: 0.727050  [ 5120/40000]\n",
      "loss: 0.804852  [ 5760/40000]\n",
      "loss: 0.804162  [ 6400/40000]\n",
      "loss: 0.755061  [ 7040/40000]\n",
      "loss: 0.791435  [ 7680/40000]\n",
      "loss: 0.820275  [ 8320/40000]\n",
      "loss: 0.743921  [ 8960/40000]\n",
      "loss: 1.114777  [ 9600/40000]\n",
      "loss: 0.764414  [10240/40000]\n",
      "loss: 0.803015  [10880/40000]\n",
      "loss: 0.748239  [11520/40000]\n",
      "loss: 0.723300  [12160/40000]\n",
      "loss: 0.904066  [12800/40000]\n",
      "loss: 0.802630  [13440/40000]\n",
      "loss: 0.744281  [14080/40000]\n",
      "loss: 0.937952  [14720/40000]\n",
      "loss: 0.711185  [15360/40000]\n",
      "loss: 0.732631  [16000/40000]\n",
      "loss: 0.695301  [16640/40000]\n",
      "loss: 0.835920  [17280/40000]\n",
      "loss: 0.781707  [17920/40000]\n",
      "loss: 0.718556  [18560/40000]\n",
      "loss: 0.703811  [19200/40000]\n",
      "loss: 1.163851  [19840/40000]\n",
      "loss: 0.782614  [20480/40000]\n",
      "loss: 0.836557  [21120/40000]\n",
      "loss: 0.697555  [21760/40000]\n",
      "loss: 0.786210  [22400/40000]\n",
      "loss: 0.813957  [23040/40000]\n",
      "loss: 1.192900  [23680/40000]\n",
      "loss: 1.020144  [24320/40000]\n",
      "loss: 0.864463  [24960/40000]\n",
      "loss: 0.939676  [25600/40000]\n",
      "loss: 0.808484  [26240/40000]\n",
      "loss: 0.746499  [26880/40000]\n",
      "loss: 3.933176  [27520/40000]\n",
      "loss: 0.838344  [28160/40000]\n",
      "loss: 0.722420  [28800/40000]\n",
      "loss: 0.729218  [29440/40000]\n",
      "loss: 1.295332  [30080/40000]\n",
      "loss: 0.741735  [30720/40000]\n",
      "loss: 0.832092  [31360/40000]\n",
      "loss: 0.733310  [32000/40000]\n",
      "loss: 0.808123  [32640/40000]\n",
      "loss: 0.767516  [33280/40000]\n",
      "loss: 0.715838  [33920/40000]\n",
      "loss: 0.728761  [34560/40000]\n",
      "loss: 0.771792  [35200/40000]\n",
      "loss: 0.712418  [35840/40000]\n",
      "loss: 1.039135  [36480/40000]\n",
      "loss: 0.787115  [37120/40000]\n",
      "loss: 1.544476  [37760/40000]\n",
      "loss: 1.002643  [38400/40000]\n",
      "loss: 0.964205  [39040/40000]\n",
      "loss: 0.780382  [39680/40000]\n",
      "Test Error: \n",
      " Accuracy: 99.6%, Avg loss: 0.837852 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.755738  [    0/40000]\n",
      "loss: 0.772161  [  640/40000]\n",
      "loss: 0.715917  [ 1280/40000]\n",
      "loss: 0.757165  [ 1920/40000]\n",
      "loss: 0.800038  [ 2560/40000]\n",
      "loss: 0.744592  [ 3200/40000]\n",
      "loss: 0.779973  [ 3840/40000]\n",
      "loss: 1.594852  [ 4480/40000]\n",
      "loss: 0.773345  [ 5120/40000]\n",
      "loss: 0.791159  [ 5760/40000]\n",
      "loss: 0.859303  [ 6400/40000]\n",
      "loss: 0.916383  [ 7040/40000]\n",
      "loss: 0.734023  [ 7680/40000]\n",
      "loss: 0.739920  [ 8320/40000]\n",
      "loss: 0.719600  [ 8960/40000]\n",
      "loss: 0.777883  [ 9600/40000]\n",
      "loss: 0.723402  [10240/40000]\n",
      "loss: 1.386374  [10880/40000]\n",
      "loss: 0.739179  [11520/40000]\n",
      "loss: 0.826921  [12160/40000]\n",
      "loss: 0.705114  [12800/40000]\n",
      "loss: 0.693565  [13440/40000]\n",
      "loss: 0.795422  [14080/40000]\n",
      "loss: 0.741603  [14720/40000]\n",
      "loss: 0.738335  [15360/40000]\n",
      "loss: 0.845790  [16000/40000]\n",
      "loss: 0.737708  [16640/40000]\n",
      "loss: 0.988450  [17280/40000]\n",
      "loss: 0.902416  [17920/40000]\n",
      "loss: 0.800757  [18560/40000]\n",
      "loss: 0.745897  [19200/40000]\n",
      "loss: 0.765308  [19840/40000]\n",
      "loss: 0.687749  [20480/40000]\n",
      "loss: 0.729177  [21120/40000]\n",
      "loss: 0.788052  [21760/40000]\n",
      "loss: 0.761196  [22400/40000]\n",
      "loss: 0.788337  [23040/40000]\n",
      "loss: 0.742535  [23680/40000]\n",
      "loss: 0.762788  [24320/40000]\n",
      "loss: 0.994449  [24960/40000]\n",
      "loss: 0.743648  [25600/40000]\n",
      "loss: 0.799113  [26240/40000]\n",
      "loss: 0.767591  [26880/40000]\n",
      "loss: 0.761998  [27520/40000]\n",
      "loss: 1.403030  [28160/40000]\n",
      "loss: 0.728011  [28800/40000]\n",
      "loss: 0.692844  [29440/40000]\n",
      "loss: 2.473275  [30080/40000]\n",
      "loss: 0.830606  [30720/40000]\n",
      "loss: 0.746462  [31360/40000]\n",
      "loss: 0.731171  [32000/40000]\n",
      "loss: 1.084831  [32640/40000]\n",
      "loss: 0.759315  [33280/40000]\n",
      "loss: 0.777967  [33920/40000]\n",
      "loss: 0.786646  [34560/40000]\n",
      "loss: 0.809565  [35200/40000]\n",
      "loss: 0.752865  [35840/40000]\n",
      "loss: 0.698584  [36480/40000]\n",
      "loss: 0.779969  [37120/40000]\n",
      "loss: 0.716089  [37760/40000]\n",
      "loss: 0.717693  [38400/40000]\n",
      "loss: 0.834132  [39040/40000]\n",
      "loss: 0.720935  [39680/40000]\n",
      "Test Error: \n",
      " Accuracy: 99.1%, Avg loss: 0.883616 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.880652  [    0/40000]\n",
      "loss: 0.785322  [  640/40000]\n",
      "loss: 0.755867  [ 1280/40000]\n",
      "loss: 0.890749  [ 1920/40000]\n",
      "loss: 0.709802  [ 2560/40000]\n",
      "loss: 0.742472  [ 3200/40000]\n",
      "loss: 0.717453  [ 3840/40000]\n",
      "loss: 0.713067  [ 4480/40000]\n",
      "loss: 0.810043  [ 5120/40000]\n",
      "loss: 1.032895  [ 5760/40000]\n",
      "loss: 0.724417  [ 6400/40000]\n",
      "loss: 0.966077  [ 7040/40000]\n",
      "loss: 0.725847  [ 7680/40000]\n",
      "loss: 0.753932  [ 8320/40000]\n",
      "loss: 0.764382  [ 8960/40000]\n",
      "loss: 0.742163  [ 9600/40000]\n",
      "loss: 0.771249  [10240/40000]\n",
      "loss: 0.701807  [10880/40000]\n",
      "loss: 1.333336  [11520/40000]\n",
      "loss: 0.732603  [12160/40000]\n",
      "loss: 0.731378  [12800/40000]\n",
      "loss: 1.114248  [13440/40000]\n",
      "loss: 0.718804  [14080/40000]\n",
      "loss: 0.777138  [14720/40000]\n",
      "loss: 0.702096  [15360/40000]\n",
      "loss: 0.707527  [16000/40000]\n",
      "loss: 0.738352  [16640/40000]\n",
      "loss: 0.820563  [17280/40000]\n",
      "loss: 0.721209  [17920/40000]\n",
      "loss: 1.154552  [18560/40000]\n",
      "loss: 0.750603  [19200/40000]\n",
      "loss: 0.786583  [19840/40000]\n",
      "loss: 0.745073  [20480/40000]\n",
      "loss: 0.735224  [21120/40000]\n",
      "loss: 0.741454  [21760/40000]\n",
      "loss: 1.575118  [22400/40000]\n",
      "loss: 0.791502  [23040/40000]\n",
      "loss: 0.739066  [23680/40000]\n",
      "loss: 0.800250  [24320/40000]\n",
      "loss: 0.708177  [24960/40000]\n",
      "loss: 0.713015  [25600/40000]\n",
      "loss: 0.787234  [26240/40000]\n",
      "loss: 0.718802  [26880/40000]\n",
      "loss: 0.775046  [27520/40000]\n",
      "loss: 0.762672  [28160/40000]\n",
      "loss: 0.740068  [28800/40000]\n",
      "loss: 0.795359  [29440/40000]\n",
      "loss: 0.802653  [30080/40000]\n",
      "loss: 0.740319  [30720/40000]\n",
      "loss: 0.756634  [31360/40000]\n",
      "loss: 0.758271  [32000/40000]\n",
      "loss: 0.717276  [32640/40000]\n",
      "loss: 0.757067  [33280/40000]\n",
      "loss: 0.727867  [33920/40000]\n",
      "loss: 0.777961  [34560/40000]\n",
      "loss: 0.769848  [35200/40000]\n",
      "loss: 0.764314  [35840/40000]\n",
      "loss: 0.721485  [36480/40000]\n",
      "loss: 0.949239  [37120/40000]\n",
      "loss: 0.862096  [37760/40000]\n",
      "loss: 0.738960  [38400/40000]\n",
      "loss: 0.728928  [39040/40000]\n",
      "loss: 0.734288  [39680/40000]\n",
      "Test Error: \n",
      " Accuracy: 101.6%, Avg loss: 0.820148 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.748997  [    0/40000]\n",
      "loss: 0.705075  [  640/40000]\n",
      "loss: 0.752617  [ 1280/40000]\n",
      "loss: 0.781598  [ 1920/40000]\n",
      "loss: 0.765008  [ 2560/40000]\n",
      "loss: 0.879326  [ 3200/40000]\n",
      "loss: 0.880455  [ 3840/40000]\n",
      "loss: 0.785605  [ 4480/40000]\n",
      "loss: 0.707081  [ 5120/40000]\n",
      "loss: 0.745444  [ 5760/40000]\n",
      "loss: 0.704150  [ 6400/40000]\n",
      "loss: 0.741593  [ 7040/40000]\n",
      "loss: 0.818877  [ 7680/40000]\n",
      "loss: 0.736596  [ 8320/40000]\n",
      "loss: 1.009268  [ 8960/40000]\n",
      "loss: 1.379698  [ 9600/40000]\n",
      "loss: 0.922699  [10240/40000]\n",
      "loss: 0.740204  [10880/40000]\n",
      "loss: 0.747005  [11520/40000]\n",
      "loss: 0.787544  [12160/40000]\n",
      "loss: 0.755043  [12800/40000]\n",
      "loss: 0.823061  [13440/40000]\n",
      "loss: 0.968292  [14080/40000]\n",
      "loss: 0.803449  [14720/40000]\n",
      "loss: 0.710843  [15360/40000]\n",
      "loss: 0.718623  [16000/40000]\n",
      "loss: 0.788804  [16640/40000]\n",
      "loss: 0.745322  [17280/40000]\n",
      "loss: 0.710836  [17920/40000]\n",
      "loss: 0.742556  [18560/40000]\n",
      "loss: 0.736402  [19200/40000]\n",
      "loss: 0.729927  [19840/40000]\n",
      "loss: 0.831280  [20480/40000]\n",
      "loss: 0.708139  [21120/40000]\n",
      "loss: 0.751966  [21760/40000]\n",
      "loss: 0.783510  [22400/40000]\n",
      "loss: 0.754494  [23040/40000]\n",
      "loss: 0.703166  [23680/40000]\n",
      "loss: 0.692386  [24320/40000]\n",
      "loss: 0.717905  [24960/40000]\n",
      "loss: 0.771635  [25600/40000]\n",
      "loss: 0.708342  [26240/40000]\n",
      "loss: 0.719552  [26880/40000]\n",
      "loss: 0.860655  [27520/40000]\n",
      "loss: 0.759197  [28160/40000]\n",
      "loss: 0.874185  [28800/40000]\n",
      "loss: 0.847562  [29440/40000]\n",
      "loss: 0.738486  [30080/40000]\n",
      "loss: 1.022151  [30720/40000]\n",
      "loss: 0.829262  [31360/40000]\n",
      "loss: 1.292403  [32000/40000]\n",
      "loss: 0.725668  [32640/40000]\n",
      "loss: 0.682945  [33280/40000]\n",
      "loss: 0.757703  [33920/40000]\n",
      "loss: 0.742200  [34560/40000]\n",
      "loss: 0.766467  [35200/40000]\n",
      "loss: 1.690248  [35840/40000]\n",
      "loss: 0.855329  [36480/40000]\n",
      "loss: 0.777311  [37120/40000]\n",
      "loss: 0.753365  [37760/40000]\n",
      "loss: 0.795244  [38400/40000]\n",
      "loss: 1.344007  [39040/40000]\n",
      "loss: 0.753009  [39680/40000]\n",
      "Test Error: \n",
      " Accuracy: 101.1%, Avg loss: 0.828003 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.715109  [    0/40000]\n",
      "loss: 0.709988  [  640/40000]\n",
      "loss: 0.765641  [ 1280/40000]\n",
      "loss: 1.436584  [ 1920/40000]\n",
      "loss: 0.775293  [ 2560/40000]\n",
      "loss: 0.733793  [ 3200/40000]\n",
      "loss: 0.747330  [ 3840/40000]\n",
      "loss: 0.971071  [ 4480/40000]\n",
      "loss: 0.726705  [ 5120/40000]\n",
      "loss: 0.906882  [ 5760/40000]\n",
      "loss: 0.902509  [ 6400/40000]\n",
      "loss: 0.744667  [ 7040/40000]\n",
      "loss: 0.715627  [ 7680/40000]\n",
      "loss: 0.716425  [ 8320/40000]\n",
      "loss: 0.888252  [ 8960/40000]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 30\u001b[0m\n\u001b[1;32m     26\u001b[0m         train_loop(train_dataloader, model, loss_fn, optimizer)\n\u001b[1;32m     27\u001b[0m         test_loop(test_dataloader, model, loss_fn)\n\u001b[0;32m---> 30\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50257\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEMBEDDING_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mHIDDEN_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTARGET_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_LAYERS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 26\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(batch_size, input_size, embedding_size, hidden_size, target_size, num_layers, epochs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 26\u001b[0m     \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     test_loop(test_dataloader, model, loss_fn)\n",
      "Cell \u001b[0;32mIn[4], line 16\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     15\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 16\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     19\u001b[0m     loss, current \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem(), batch \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(X)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    381\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    382\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    383\u001b[0m             )\n\u001b[0;32m--> 385\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    388\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:166\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    155\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    157\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    158\u001b[0m         group,\n\u001b[1;32m    159\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    163\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    164\u001b[0m         state_steps)\n\u001b[0;32m--> 166\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:316\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    314\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 316\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:439\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    437\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 439\u001b[0m         denom \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbias_correction2_sqrt\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    441\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[1;32m    443\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main(batch_size, input_size, embedding_size, hidden_size, target_size, num_layers, epochs=None):\n",
    "    dataset = pd.read_csv(\n",
    "        \"../datasets/IMDB-Dataset.csv\", usecols=[\"review\", \"sentiment\"]\n",
    "    )\n",
    "    dataset[\"sentiment\"] = dataset[\"sentiment\"].map({\"positive\": 1, \"negative\": 0})\n",
    "    train, test = train_test_split(dataset, test_size=0.2)\n",
    "    learning_rate = 0.003\n",
    "\n",
    "    encoder = nn.Transformer(d_model=512, nhead=8, dim_feedforward=2048, dropout=0.1, activation=\"gelu\", custom_encoder=None, custom_decoder=None, layer_norm_eps=1e-05, batch_first=False, norm_first=False, bias=True, device=None, dtype=None).encoder.layers[0]\n",
    "    model = CustomEmbedding(encoder, input_size, embedding_size, hidden_size, target_size)\n",
    "    \n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_dataset = MoviesDataset(train['review'].values, train['sentiment'].values, model_max_length=num_layers)\n",
    "    test_dataset = MoviesDataset(test['review'].values, test['sentiment'].values, model_max_length=num_layers)\n",
    "\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size , shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size, shuffle=True)\n",
    "\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "\n",
    "        train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "        test_loop(test_dataloader, model, loss_fn)\n",
    "\n",
    "\n",
    "main(\n",
    "    batch_size=BATCH_SIZE,\n",
    "    input_size=50257,\n",
    "    embedding_size=EMBEDDING_SIZE,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    target_size=TARGET_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    epochs=10,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
