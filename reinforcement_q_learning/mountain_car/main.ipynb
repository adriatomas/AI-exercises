{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import gym\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuralnetwork(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size = 128):\n",
    "        super().__init__()\n",
    "        self.sequetial_stack = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sequetial_stack(x)\n",
    "        return x\n",
    "\n",
    "    def save(self, file_name=f'model{random.randint(0,10000)}.pth'):\n",
    "        model_folder_path = './model'\n",
    "        \n",
    "        if not os.path.exists(model_folder_path):\n",
    "            os.makedirs(model_folder_path)\n",
    "\n",
    "        file_path = os.path.join(model_folder_path, file_name)\n",
    "\n",
    "        torch.save(self.state_dict(), file_path)\n",
    "\n",
    "class QTrainer:\n",
    "    def __init__(self, model, lr, gamma):\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.model = model\n",
    "        self.optimizer = optim.Adam(model.parameters(), lr=self.lr)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def train_step(self, state, action, reward, next_state, done):\n",
    "        state = torch.tensor(state, dtype=torch.float)\n",
    "        next_state = torch.tensor(next_state, dtype=torch.float)\n",
    "        action = torch.tensor(action, dtype=torch.long)\n",
    "        reward = torch.tensor(reward, dtype=torch.float)\n",
    "        # (n, x)\n",
    "\n",
    "        if len(state.shape) == 1:\n",
    "            # (1, x)\n",
    "            state = torch.unsqueeze(state, 0)\n",
    "            next_state = torch.unsqueeze(next_state, 0)\n",
    "            action = torch.unsqueeze(action, 0)\n",
    "            reward = torch.unsqueeze(reward, 0)\n",
    "            done = (done, )\n",
    "\n",
    "        # 1: predicted Q values with current state\n",
    "        pred = self.model(state)\n",
    "\n",
    "        target = pred.clone()\n",
    "        for idx in range(len(done)):\n",
    "            q_new = reward[idx]\n",
    "            if not done[idx]:\n",
    "                q_new = reward[idx] + self.gamma * torch.max(self.model(next_state[idx]))\n",
    "\n",
    "            target[idx][torch.argmax(action[idx]).item()] = q_new\n",
    "    \n",
    "        # 2: Q_new = r + y * max(next_predicted Q value) -> only do this if not done\n",
    "        # pred.clone()\n",
    "        # preds[argmax(action)] = Q_new\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = self.criterion(target, pred)\n",
    "        loss.backward()\n",
    "\n",
    "        self.optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "def plot(scores, mean_scores):\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "    plt.clf()\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Number of Games')\n",
    "    plt.ylabel('Score')\n",
    "    plt.plot(scores)\n",
    "    plt.plot(mean_scores)\n",
    "    plt.ylim(ymin=0)\n",
    "    plt.text(len(scores)-1, scores[-1], str(scores[-1]))\n",
    "    plt.text(len(mean_scores)-1, mean_scores[-1], str(mean_scores[-1]))\n",
    "    plt.show(block=False)\n",
    "    plt.pause(.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_MEMORY = 100_000\n",
    "BATCH_SIZE = 1000\n",
    "LR = 0.001\n",
    "\n",
    "class Agent:\n",
    "  \n",
    "  def __init__(self, input_size, hidden_size, output_size) -> None:\n",
    "    self.n_games = 0\n",
    "    self.epsilon = 0 # randomness\n",
    "    self.gamma = 0.9 # discount rate\n",
    "    self.memory = deque(maxlen=MAX_MEMORY)\n",
    "    self.model = Neuralnetwork(input_size,output_size ,hidden_size)\n",
    "    self.trainer = QTrainer(self.model, lr=LR, gamma=self.gamma)\n",
    "\n",
    "  def get_state(self, state):\n",
    "    # is not need right now for our game but it is good to have it\n",
    "    pass\n",
    "\n",
    "  def get_action(self, state) -> list[int]:\n",
    "    # self.epsilon = 80 - self.n_games\n",
    "    self.epsilon = 90 - self.n_games*(80/200)\n",
    "    self.epsilon = 10 if self.epsilon < 10 else self.epsilon\n",
    "    final_move = None\n",
    "\n",
    "    if (random.randint(0, 100) < self.epsilon):\n",
    "        final_move = random.randint(0, 1)\n",
    "    else:\n",
    "        state0 = torch.tensor(state, dtype=torch.float)\n",
    "        prediction = self.model(state0)\n",
    "        final_move = torch.argmax(prediction).item()\n",
    "        print(prediction)\n",
    "        # print('final_model', final_move)\n",
    "        \n",
    "    return final_move\n",
    "    \n",
    "  def remember(self, state, action, reward, next_state, done) -> None:\n",
    "    self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "  def train_short_memory(self, state, action, reward, next_state, done) -> None:\n",
    "    self.trainer.train_step(state, action, reward, next_state, done)\n",
    "\n",
    "  \n",
    "  def train_long_memory(self) -> None:\n",
    "      if len(self.memory) > BATCH_SIZE:\n",
    "          mini_sample = random.sample(self.memory, BATCH_SIZE) # list of tuples\n",
    "      else:\n",
    "          mini_sample = self.memory\n",
    "\n",
    "      states, actions, rewards, next_states, dones = zip(*mini_sample)\n",
    "      self.trainer.train_step(states, actions, rewards, next_states, dones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game number: 26\n",
      "else 79.6\n",
      "tensor([2.5756, 0.0550], grad_fn=<ViewBackward0>)\n",
      "Reward:  1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_588532/3192077506.py\", line 41, in <module>\n",
      "    start()\n",
      "  File \"/tmp/ipykernel_588532/3192077506.py\", line 33, in start\n",
      "    agent.train_long_memory()\n",
      "  File \"/tmp/ipykernel_588532/2785920552.py\", line 51, in train_long_memory\n",
      "    self.trainer.train_step(states, actions, rewards, next_states, dones)\n",
      "  File \"/tmp/ipykernel_588532/2898612456.py\", line 65, in train_step\n",
      "    loss.backward()\n",
      "  File \"/home/adria/.local/lib/python3.10/site-packages/torch/_tensor.py\", line 522, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/home/adria/.local/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 266, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "def start() -> None:\n",
    "  env = gym.make('CartPole-v1', render_mode='human')\n",
    "  # 0 Cart position -4.8 to 4.8\n",
    "  # 1 Cart velocity -inf to inf\n",
    "  # 2 Pole angle ~ -0.418 rad (-24°) to 0.418 rad (24°)\n",
    "  # 3 Pole velocity at tip -inf to inf\n",
    "  n_actions = env.action_space.n # output\n",
    "  state, info = env.reset()\n",
    "  n_observations = len(state) ## input\n",
    "  total_score = 0\n",
    "  record = 0\n",
    "  \n",
    "  agent = Agent(n_observations, 128, n_actions)\n",
    "\n",
    "  while True:\n",
    "    env.render()\n",
    "    display.clear_output(wait=True)\n",
    "    print(f'Game number: {agent.n_games}')\n",
    "    state_old = state\n",
    "    final_move = agent.get_action(state_old) # [0, 1]\n",
    "\n",
    "    curr_state, reward, terminated, truncated, _ = env.step(final_move)\n",
    "    print('Reward: ', reward)\n",
    "    state = curr_state\n",
    "\n",
    "    agent.train_short_memory(state_old, final_move, reward, state, terminated)\n",
    "\n",
    "    agent.remember(state_old, final_move, reward, state, terminated)\n",
    "\n",
    "    if terminated:\n",
    "      env.reset()\n",
    "      agent.n_games += 1\n",
    "      agent.train_long_memory()\n",
    "\n",
    "      if total_score > record:\n",
    "        record = total_score\n",
    "        agent.model.save()\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "  try:       \n",
    "    start()\n",
    "  except:\n",
    "    traceback.print_exc()\n",
    "    env.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
